{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f91e45b-0cc8-472f-8183-c03b945531f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "y_train shape: (50000, 1)\n",
      "Learning rate:  0.1\n",
      "Model: \"model_25\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_26 (InputLayer)          [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_1101 (Conv2D)           (None, 32, 32, 16)   448         ['input_26[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_1051 (Batc  (None, 32, 32, 16)  64          ['conv2d_1101[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " activation_1051 (Activation)   (None, 32, 32, 16)   0           ['batch_normalization_1051[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv2d_1102 (Conv2D)           (None, 32, 32, 16)   2320        ['activation_1051[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1052 (Batc  (None, 32, 32, 16)  64          ['conv2d_1102[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " activation_1052 (Activation)   (None, 32, 32, 16)   0           ['batch_normalization_1052[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv2d_1103 (Conv2D)           (None, 32, 32, 16)   2320        ['activation_1052[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1053 (Batc  (None, 32, 32, 16)  64          ['conv2d_1103[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " add_513 (Add)                  (None, 32, 32, 16)   0           ['activation_1051[0][0]',        \n",
      "                                                                  'batch_normalization_1053[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " activation_1053 (Activation)   (None, 32, 32, 16)   0           ['add_513[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1104 (Conv2D)           (None, 32, 32, 16)   2320        ['activation_1053[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1054 (Batc  (None, 32, 32, 16)  64          ['conv2d_1104[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " activation_1054 (Activation)   (None, 32, 32, 16)   0           ['batch_normalization_1054[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv2d_1105 (Conv2D)           (None, 32, 32, 16)   2320        ['activation_1054[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1055 (Batc  (None, 32, 32, 16)  64          ['conv2d_1105[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " add_514 (Add)                  (None, 32, 32, 16)   0           ['activation_1053[0][0]',        \n",
      "                                                                  'batch_normalization_1055[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " activation_1055 (Activation)   (None, 32, 32, 16)   0           ['add_514[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1106 (Conv2D)           (None, 32, 32, 16)   2320        ['activation_1055[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1056 (Batc  (None, 32, 32, 16)  64          ['conv2d_1106[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " activation_1056 (Activation)   (None, 32, 32, 16)   0           ['batch_normalization_1056[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv2d_1107 (Conv2D)           (None, 32, 32, 16)   2320        ['activation_1056[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1057 (Batc  (None, 32, 32, 16)  64          ['conv2d_1107[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " add_515 (Add)                  (None, 32, 32, 16)   0           ['activation_1055[0][0]',        \n",
      "                                                                  'batch_normalization_1057[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " activation_1057 (Activation)   (None, 32, 32, 16)   0           ['add_515[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1108 (Conv2D)           (None, 32, 32, 16)   2320        ['activation_1057[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1058 (Batc  (None, 32, 32, 16)  64          ['conv2d_1108[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " activation_1058 (Activation)   (None, 32, 32, 16)   0           ['batch_normalization_1058[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv2d_1109 (Conv2D)           (None, 32, 32, 16)   2320        ['activation_1058[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1059 (Batc  (None, 32, 32, 16)  64          ['conv2d_1109[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " add_516 (Add)                  (None, 32, 32, 16)   0           ['activation_1057[0][0]',        \n",
      "                                                                  'batch_normalization_1059[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " activation_1059 (Activation)   (None, 32, 32, 16)   0           ['add_516[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1110 (Conv2D)           (None, 32, 32, 16)   2320        ['activation_1059[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1060 (Batc  (None, 32, 32, 16)  64          ['conv2d_1110[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " activation_1060 (Activation)   (None, 32, 32, 16)   0           ['batch_normalization_1060[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv2d_1111 (Conv2D)           (None, 32, 32, 16)   2320        ['activation_1060[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1061 (Batc  (None, 32, 32, 16)  64          ['conv2d_1111[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " add_517 (Add)                  (None, 32, 32, 16)   0           ['activation_1059[0][0]',        \n",
      "                                                                  'batch_normalization_1061[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " activation_1061 (Activation)   (None, 32, 32, 16)   0           ['add_517[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1112 (Conv2D)           (None, 32, 32, 16)   2320        ['activation_1061[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1062 (Batc  (None, 32, 32, 16)  64          ['conv2d_1112[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " activation_1062 (Activation)   (None, 32, 32, 16)   0           ['batch_normalization_1062[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv2d_1113 (Conv2D)           (None, 32, 32, 16)   2320        ['activation_1062[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1063 (Batc  (None, 32, 32, 16)  64          ['conv2d_1113[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " add_518 (Add)                  (None, 32, 32, 16)   0           ['activation_1061[0][0]',        \n",
      "                                                                  'batch_normalization_1063[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " activation_1063 (Activation)   (None, 32, 32, 16)   0           ['add_518[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1114 (Conv2D)           (None, 32, 32, 16)   2320        ['activation_1063[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1064 (Batc  (None, 32, 32, 16)  64          ['conv2d_1114[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " activation_1064 (Activation)   (None, 32, 32, 16)   0           ['batch_normalization_1064[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv2d_1115 (Conv2D)           (None, 32, 32, 16)   2320        ['activation_1064[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1065 (Batc  (None, 32, 32, 16)  64          ['conv2d_1115[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " add_519 (Add)                  (None, 32, 32, 16)   0           ['activation_1063[0][0]',        \n",
      "                                                                  'batch_normalization_1065[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " activation_1065 (Activation)   (None, 32, 32, 16)   0           ['add_519[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1116 (Conv2D)           (None, 16, 16, 32)   4640        ['activation_1065[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1066 (Batc  (None, 16, 16, 32)  128         ['conv2d_1116[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " activation_1066 (Activation)   (None, 16, 16, 32)   0           ['batch_normalization_1066[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv2d_1117 (Conv2D)           (None, 16, 16, 32)   9248        ['activation_1066[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_1118 (Conv2D)           (None, 16, 16, 32)   544         ['activation_1065[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1067 (Batc  (None, 16, 16, 32)  128         ['conv2d_1117[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " add_520 (Add)                  (None, 16, 16, 32)   0           ['conv2d_1118[0][0]',            \n",
      "                                                                  'batch_normalization_1067[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " activation_1067 (Activation)   (None, 16, 16, 32)   0           ['add_520[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1119 (Conv2D)           (None, 16, 16, 32)   9248        ['activation_1067[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1068 (Batc  (None, 16, 16, 32)  128         ['conv2d_1119[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " activation_1068 (Activation)   (None, 16, 16, 32)   0           ['batch_normalization_1068[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv2d_1120 (Conv2D)           (None, 16, 16, 32)   9248        ['activation_1068[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1069 (Batc  (None, 16, 16, 32)  128         ['conv2d_1120[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " add_521 (Add)                  (None, 16, 16, 32)   0           ['activation_1067[0][0]',        \n",
      "                                                                  'batch_normalization_1069[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " activation_1069 (Activation)   (None, 16, 16, 32)   0           ['add_521[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1121 (Conv2D)           (None, 16, 16, 32)   9248        ['activation_1069[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1070 (Batc  (None, 16, 16, 32)  128         ['conv2d_1121[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " activation_1070 (Activation)   (None, 16, 16, 32)   0           ['batch_normalization_1070[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv2d_1122 (Conv2D)           (None, 16, 16, 32)   9248        ['activation_1070[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1071 (Batc  (None, 16, 16, 32)  128         ['conv2d_1122[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " add_522 (Add)                  (None, 16, 16, 32)   0           ['activation_1069[0][0]',        \n",
      "                                                                  'batch_normalization_1071[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " activation_1071 (Activation)   (None, 16, 16, 32)   0           ['add_522[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1123 (Conv2D)           (None, 16, 16, 32)   9248        ['activation_1071[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1072 (Batc  (None, 16, 16, 32)  128         ['conv2d_1123[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " activation_1072 (Activation)   (None, 16, 16, 32)   0           ['batch_normalization_1072[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv2d_1124 (Conv2D)           (None, 16, 16, 32)   9248        ['activation_1072[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1073 (Batc  (None, 16, 16, 32)  128         ['conv2d_1124[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " add_523 (Add)                  (None, 16, 16, 32)   0           ['activation_1071[0][0]',        \n",
      "                                                                  'batch_normalization_1073[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " activation_1073 (Activation)   (None, 16, 16, 32)   0           ['add_523[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1125 (Conv2D)           (None, 16, 16, 32)   9248        ['activation_1073[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1074 (Batc  (None, 16, 16, 32)  128         ['conv2d_1125[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " activation_1074 (Activation)   (None, 16, 16, 32)   0           ['batch_normalization_1074[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv2d_1126 (Conv2D)           (None, 16, 16, 32)   9248        ['activation_1074[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1075 (Batc  (None, 16, 16, 32)  128         ['conv2d_1126[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " add_524 (Add)                  (None, 16, 16, 32)   0           ['activation_1073[0][0]',        \n",
      "                                                                  'batch_normalization_1075[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " activation_1075 (Activation)   (None, 16, 16, 32)   0           ['add_524[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1127 (Conv2D)           (None, 16, 16, 32)   9248        ['activation_1075[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1076 (Batc  (None, 16, 16, 32)  128         ['conv2d_1127[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " activation_1076 (Activation)   (None, 16, 16, 32)   0           ['batch_normalization_1076[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv2d_1128 (Conv2D)           (None, 16, 16, 32)   9248        ['activation_1076[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1077 (Batc  (None, 16, 16, 32)  128         ['conv2d_1128[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " add_525 (Add)                  (None, 16, 16, 32)   0           ['activation_1075[0][0]',        \n",
      "                                                                  'batch_normalization_1077[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " activation_1077 (Activation)   (None, 16, 16, 32)   0           ['add_525[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1129 (Conv2D)           (None, 16, 16, 32)   9248        ['activation_1077[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1078 (Batc  (None, 16, 16, 32)  128         ['conv2d_1129[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " activation_1078 (Activation)   (None, 16, 16, 32)   0           ['batch_normalization_1078[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv2d_1130 (Conv2D)           (None, 16, 16, 32)   9248        ['activation_1078[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1079 (Batc  (None, 16, 16, 32)  128         ['conv2d_1130[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " add_526 (Add)                  (None, 16, 16, 32)   0           ['activation_1077[0][0]',        \n",
      "                                                                  'batch_normalization_1079[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " activation_1079 (Activation)   (None, 16, 16, 32)   0           ['add_526[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1131 (Conv2D)           (None, 8, 8, 64)     18496       ['activation_1079[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1080 (Batc  (None, 8, 8, 64)    256         ['conv2d_1131[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " activation_1080 (Activation)   (None, 8, 8, 64)     0           ['batch_normalization_1080[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv2d_1132 (Conv2D)           (None, 8, 8, 64)     36928       ['activation_1080[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_1133 (Conv2D)           (None, 8, 8, 64)     2112        ['activation_1079[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1081 (Batc  (None, 8, 8, 64)    256         ['conv2d_1132[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " add_527 (Add)                  (None, 8, 8, 64)     0           ['conv2d_1133[0][0]',            \n",
      "                                                                  'batch_normalization_1081[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " activation_1081 (Activation)   (None, 8, 8, 64)     0           ['add_527[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1134 (Conv2D)           (None, 8, 8, 64)     36928       ['activation_1081[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1082 (Batc  (None, 8, 8, 64)    256         ['conv2d_1134[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " activation_1082 (Activation)   (None, 8, 8, 64)     0           ['batch_normalization_1082[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv2d_1135 (Conv2D)           (None, 8, 8, 64)     36928       ['activation_1082[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1083 (Batc  (None, 8, 8, 64)    256         ['conv2d_1135[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " add_528 (Add)                  (None, 8, 8, 64)     0           ['activation_1081[0][0]',        \n",
      "                                                                  'batch_normalization_1083[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " activation_1083 (Activation)   (None, 8, 8, 64)     0           ['add_528[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1136 (Conv2D)           (None, 8, 8, 64)     36928       ['activation_1083[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1084 (Batc  (None, 8, 8, 64)    256         ['conv2d_1136[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " activation_1084 (Activation)   (None, 8, 8, 64)     0           ['batch_normalization_1084[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv2d_1137 (Conv2D)           (None, 8, 8, 64)     36928       ['activation_1084[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1085 (Batc  (None, 8, 8, 64)    256         ['conv2d_1137[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " add_529 (Add)                  (None, 8, 8, 64)     0           ['activation_1083[0][0]',        \n",
      "                                                                  'batch_normalization_1085[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " activation_1085 (Activation)   (None, 8, 8, 64)     0           ['add_529[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1138 (Conv2D)           (None, 8, 8, 64)     36928       ['activation_1085[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1086 (Batc  (None, 8, 8, 64)    256         ['conv2d_1138[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " activation_1086 (Activation)   (None, 8, 8, 64)     0           ['batch_normalization_1086[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv2d_1139 (Conv2D)           (None, 8, 8, 64)     36928       ['activation_1086[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1087 (Batc  (None, 8, 8, 64)    256         ['conv2d_1139[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " add_530 (Add)                  (None, 8, 8, 64)     0           ['activation_1085[0][0]',        \n",
      "                                                                  'batch_normalization_1087[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " activation_1087 (Activation)   (None, 8, 8, 64)     0           ['add_530[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1140 (Conv2D)           (None, 8, 8, 64)     36928       ['activation_1087[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1088 (Batc  (None, 8, 8, 64)    256         ['conv2d_1140[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " activation_1088 (Activation)   (None, 8, 8, 64)     0           ['batch_normalization_1088[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv2d_1141 (Conv2D)           (None, 8, 8, 64)     36928       ['activation_1088[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1089 (Batc  (None, 8, 8, 64)    256         ['conv2d_1141[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " add_531 (Add)                  (None, 8, 8, 64)     0           ['activation_1087[0][0]',        \n",
      "                                                                  'batch_normalization_1089[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " activation_1089 (Activation)   (None, 8, 8, 64)     0           ['add_531[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1142 (Conv2D)           (None, 8, 8, 64)     36928       ['activation_1089[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1090 (Batc  (None, 8, 8, 64)    256         ['conv2d_1142[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " activation_1090 (Activation)   (None, 8, 8, 64)     0           ['batch_normalization_1090[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv2d_1143 (Conv2D)           (None, 8, 8, 64)     36928       ['activation_1090[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1091 (Batc  (None, 8, 8, 64)    256         ['conv2d_1143[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " add_532 (Add)                  (None, 8, 8, 64)     0           ['activation_1089[0][0]',        \n",
      "                                                                  'batch_normalization_1091[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " activation_1091 (Activation)   (None, 8, 8, 64)     0           ['add_532[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1144 (Conv2D)           (None, 8, 8, 64)     36928       ['activation_1091[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1092 (Batc  (None, 8, 8, 64)    256         ['conv2d_1144[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " activation_1092 (Activation)   (None, 8, 8, 64)     0           ['batch_normalization_1092[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv2d_1145 (Conv2D)           (None, 8, 8, 64)     36928       ['activation_1092[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1093 (Batc  (None, 8, 8, 64)    256         ['conv2d_1145[0][0]']            \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " add_533 (Add)                  (None, 8, 8, 64)     0           ['activation_1091[0][0]',        \n",
      "                                                                  'batch_normalization_1093[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " activation_1093 (Activation)   (None, 8, 8, 64)     0           ['add_533[0][0]']                \n",
      "                                                                                                  \n",
      " average_pooling2d_25 (AverageP  (None, 1, 1, 64)    0           ['activation_1093[0][0]']        \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " flatten_25 (Flatten)           (None, 64)           0           ['average_pooling2d_25[0][0]']   \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 10)           650         ['flatten_25[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 665,994\n",
      "Trainable params: 662,826\n",
      "Non-trainable params: 3,168\n",
      "__________________________________________________________________________________________________\n",
      "ResNet44v1\n",
      "Not using data augmentation.\n",
      "Learning rate:  0.1\n",
      "Epoch 1/10\n",
      "   83/50000 [..............................] - ETA: 32:11 - loss: 5.3884 - accuracy: 0.0843"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [37], line 273\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data_augmentation:\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNot using data augmentation.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 273\u001b[0m     hist \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m              \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m              \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m              \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m              \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m              \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUsing real-time data augmentation.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DiscreteMath/lib/python3.8/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DiscreteMath/lib/python3.8/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DiscreteMath/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DiscreteMath/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DiscreteMath/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DiscreteMath/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DiscreteMath/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DiscreteMath/lib/python3.8/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DiscreteMath/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.datasets import cifar10\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 1  # orig paper trained all networks with batch_size=128\n",
    "epochs = 10\n",
    "data_augmentation = False\n",
    "num_classes = 10\n",
    "\n",
    "# Subtracting pixel mean improves accuracy\n",
    "subtract_pixel_mean = True\n",
    "\n",
    "# Model parameter\n",
    "# ----------------------------------------------------------------------------\n",
    "#           |      | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch\n",
    "# Model     |  n   | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti\n",
    "#           |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2)\n",
    "# ----------------------------------------------------------------------------\n",
    "# ResNet20  | 3 (2)| 92.16     | 91.25     | -----     | -----     | 35 (---)\n",
    "# ResNet32  | 5(NA)| 92.46     | 92.49     | NA        | NA        | 50 ( NA)\n",
    "# ResNet44  | 7(NA)| 92.50     | 92.83     | NA        | NA        | 70 ( NA)\n",
    "# ResNet56  | 9 (6)| 92.71     | 93.03     | 93.01     | NA        | 90 (100)\n",
    "# ResNet110 |18(12)| 92.65     | 93.39+-.16| 93.15     | 93.63     | 165(180)\n",
    "# ResNet164 |27(18)| -----     | 94.07     | -----     | 94.54     | ---(---)\n",
    "# ResNet1001| (111)| -----     | 92.39     | -----     | 95.08+-.14| ---(---)\n",
    "# ---------------------------------------------------------------------------\n",
    "n = 7\n",
    "\n",
    "# Model version\n",
    "# Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n",
    "version = 1\n",
    "\n",
    "# Computed depth from supplied model parameter n\n",
    "if version == 1:\n",
    "    depth = n * 6 + 2\n",
    "elif version == 2:\n",
    "    depth = n * 9 + 2\n",
    "\n",
    "# Model name, depth and version\n",
    "model_type = 'ResNet%dv%d' % (depth, version)\n",
    "\n",
    "# Load the CIFAR10 data.\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Input image dimensions.\n",
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# If subtract pixel mean is enabled\n",
    "if subtract_pixel_mean:\n",
    "    x_train_mean = np.mean(x_train, axis=0)\n",
    "    x_train -= x_train_mean\n",
    "    x_test -= x_train_mean\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print('y_train shape:', y_train.shape)\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "loss_per_epoch = []\n",
    "loss_per_batch = []\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses_batch = []\n",
    "        self.val_losses_batch = []\n",
    "        self.losses_epoch = []\n",
    "        self.val_losses_epoch = []\n",
    "        \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses_batch.append(logs.get('loss'))\n",
    "        self.val_losses_batch.append(logs.get('val_loss'))\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.losses_epoch.append(logs.get('loss'))\n",
    "        self.val_losses_epoch.append(logs.get('val_loss'))\n",
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "\n",
    "    Learning rate is scheduled to be reduced after 100, 200, 300 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-1\n",
    "    if epoch > 300:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 200:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 100:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr\n",
    "\n",
    "\n",
    "\n",
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
    "\n",
    "    # Arguments\n",
    "        inputs (tensor): input tensor from input image or previous layer\n",
    "        num_filters (int): Conv2D number of filters\n",
    "        kernel_size (int): Conv2D square kernel dimensions\n",
    "        strides (int): Conv2D square stride dimensions\n",
    "        activation (string): activation name\n",
    "        batch_normalization (bool): whether to include batch normalization\n",
    "        conv_first (bool): conv-bn-activation (True) or\n",
    "            bn-activation-conv (False)\n",
    "\n",
    "    # Returns\n",
    "        x (tensor): tensor as input to the next layer\n",
    "    \"\"\"\n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def resnet_v1(input_shape, depth, num_classes=10):\n",
    "    \"\"\"ResNet Version 1 Model builder [a]\n",
    "\n",
    "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
    "    Last ReLU is after the shortcut connection.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filters is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same number of filters.\n",
    "    Features maps sizes:\n",
    "    stage 0: 32x32, 16\n",
    "    stage 1: 16x16, 32\n",
    "    stage 2:  8x8,  64\n",
    "    The Number of parameters is approx the same as Table 6 of [a]:\n",
    "    ResNet20 0.27M\n",
    "    ResNet32 0.46M\n",
    "    ResNet44 0.66M\n",
    "    ResNet56 0.85M\n",
    "    ResNet110 1.7M\n",
    "\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 6 != 0:\n",
    "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
    "    # Start model definition.\n",
    "    num_filters = 16\n",
    "    num_res_blocks = int((depth - 2) / 6)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = resnet_layer(inputs=inputs)\n",
    "    # Instantiate the stack of residual units\n",
    "    for stack in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            strides = 1\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                strides = 2  # downsample\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters,\n",
    "                             strides=strides)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters,\n",
    "                             activation=None)\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y])\n",
    "            x = Activation('relu')(x)\n",
    "        num_filters *= 2\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v1 does not use BN after last shortcut connection-ReLU\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "    \n",
    "model = resnet_v1(input_shape=input_shape, depth=depth)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(learning_rate=lr_schedule(0)),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "print(model_type)\n",
    "\n",
    "# Prepare model model saving directory.\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "filepath = os.path.join(save_dir, model_name)\n",
    "\n",
    "# Prepare callbacks for model saving and for learning rate adjustment.\n",
    "checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                             monitor='val_acc',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "history = LossHistory()\n",
    "tb_callback = tf.keras.callbacks.TensorBoard('./logs_Q4/v100', update_freq=1)\n",
    "callbacks = [checkpoint, lr_reducer, lr_scheduler, tb_callback, history]\n",
    "\n",
    "# Run training, with or without data augmentation.\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    hist = model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True,\n",
    "              callbacks=callbacks)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        # set input mean to 0 over the dataset\n",
    "        featurewise_center=False,\n",
    "        # set each sample mean to 0\n",
    "        samplewise_center=False,\n",
    "        # divide inputs by std of dataset\n",
    "        featurewise_std_normalization=False,\n",
    "        # divide each input by its std\n",
    "        samplewise_std_normalization=False,\n",
    "        # apply ZCA whitening\n",
    "        zca_whitening=False,\n",
    "        # epsilon for ZCA whitening\n",
    "        zca_epsilon=1e-06,\n",
    "        # randomly rotate images in the range (deg 0 to 180)\n",
    "        rotation_range=0,\n",
    "        # randomly shift images horizontally\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically\n",
    "        height_shift_range=0.1,\n",
    "        # set range for random shear\n",
    "        shear_range=0.,\n",
    "        # set range for random zoom\n",
    "        zoom_range=0.,\n",
    "        # set range for random channel shifts\n",
    "        channel_shift_range=0.,\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        # value used for fill_mode = \"constant\"\n",
    "        cval=0.,\n",
    "        # randomly flip images\n",
    "        horizontal_flip=True,\n",
    "        # randomly flip images\n",
    "        vertical_flip=False,\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "\n",
    "    # Compute quantities required for featurewise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        epochs=epochs, verbose=1, workers=4,\n",
    "                        callbacks=callbacks)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8330b094-8ed1-48f9-9532-88f2a64c5bed",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14.203859329223633,\n",
       " 7.270102500915527,\n",
       " 19.98906135559082,\n",
       " 15.076791763305664,\n",
       " 18.13641929626465,\n",
       " 16.30466651916504,\n",
       " 18.44782829284668,\n",
       " 17.12566566467285,\n",
       " 15.261430740356445,\n",
       " 14.649927139282227,\n",
       " 14.449054718017578,\n",
       " 13.62307071685791,\n",
       " 13.213071823120117,\n",
       " 12.796809196472168,\n",
       " 12.748737335205078,\n",
       " 12.291885375976562,\n",
       " 12.072188377380371,\n",
       " 11.67867660522461,\n",
       " 11.248303413391113,\n",
       " 11.00451374053955,\n",
       " 10.615013122558594,\n",
       " 10.412745475769043,\n",
       " 10.142382621765137,\n",
       " 10.005167961120605,\n",
       " 9.716453552246094,\n",
       " 9.501852035522461,\n",
       " 9.188719749450684,\n",
       " 9.012530326843262,\n",
       " 8.766111373901367,\n",
       " 8.643162727355957,\n",
       " 8.491058349609375,\n",
       " 8.292279243469238,\n",
       " 8.160934448242188,\n",
       " 8.04251480102539,\n",
       " 7.907651424407959,\n",
       " 7.730448246002197,\n",
       " 7.637423992156982,\n",
       " 7.517452239990234,\n",
       " 7.39885950088501,\n",
       " 7.289341926574707,\n",
       " 7.183862686157227,\n",
       " 7.097285747528076,\n",
       " 6.976309776306152,\n",
       " 6.892190933227539,\n",
       " 6.832052230834961,\n",
       " 6.766961097717285,\n",
       " 6.67992639541626,\n",
       " 6.613738536834717,\n",
       " 6.533090591430664,\n",
       " 6.485581874847412,\n",
       " 6.405803680419922,\n",
       " 6.345708847045898,\n",
       " 6.289645195007324,\n",
       " 6.230306148529053,\n",
       " 6.167767524719238,\n",
       " 6.109906196594238,\n",
       " 6.048406600952148,\n",
       " 6.012803554534912,\n",
       " 5.948827266693115,\n",
       " 5.898406028747559,\n",
       " 5.83854866027832,\n",
       " 5.77634859085083,\n",
       " 5.737244606018066,\n",
       " 5.702363014221191,\n",
       " 5.646870136260986,\n",
       " 5.606823921203613,\n",
       " 5.5745625495910645,\n",
       " 5.5311598777771,\n",
       " 5.5104827880859375,\n",
       " 5.481922149658203,\n",
       " 5.431176662445068,\n",
       " 5.417818069458008,\n",
       " 5.379030227661133,\n",
       " 5.3326873779296875,\n",
       " 5.301370620727539,\n",
       " 5.2701263427734375,\n",
       " 5.229084014892578,\n",
       " 5.209989070892334,\n",
       " 5.181584358215332,\n",
       " 5.1566033363342285,\n",
       " 5.119358539581299,\n",
       " 5.083850860595703,\n",
       " 5.081202983856201,\n",
       " 5.0396013259887695,\n",
       " 5.023153305053711,\n",
       " 5.004394054412842,\n",
       " 4.969323635101318,\n",
       " 4.943336009979248,\n",
       " 4.918445110321045,\n",
       " 4.886222839355469,\n",
       " 4.856240272521973,\n",
       " 4.848585605621338,\n",
       " 4.826421737670898,\n",
       " 4.808950424194336,\n",
       " 4.798927307128906,\n",
       " 4.773845195770264,\n",
       " 4.752971649169922,\n",
       " 4.731593608856201,\n",
       " 4.707818031311035,\n",
       " 4.6977972984313965,\n",
       " 3.2619359493255615,\n",
       " 3.0738229751586914,\n",
       " 3.184904098510742,\n",
       " 3.201788902282715,\n",
       " 3.00213623046875,\n",
       " 2.927813768386841,\n",
       " 2.9368762969970703,\n",
       " 2.907688856124878,\n",
       " 2.886488437652588,\n",
       " 2.8109002113342285,\n",
       " 2.796658515930176,\n",
       " 2.702564239501953,\n",
       " 2.6925313472747803,\n",
       " 2.7233002185821533,\n",
       " 2.687084436416626,\n",
       " 2.6091489791870117,\n",
       " 2.606961488723755,\n",
       " 2.675917625427246,\n",
       " 2.6823575496673584,\n",
       " 2.6599721908569336,\n",
       " 2.7102603912353516,\n",
       " 2.7312474250793457,\n",
       " 2.767749547958374,\n",
       " 2.8117988109588623,\n",
       " 2.7876741886138916,\n",
       " 2.802215099334717,\n",
       " 2.82680344581604,\n",
       " 2.796217441558838,\n",
       " 2.804424524307251,\n",
       " 2.769972801208496,\n",
       " 2.778203248977661,\n",
       " 2.779205322265625,\n",
       " 2.7706079483032227,\n",
       " 2.788477659225464,\n",
       " 2.790346145629883,\n",
       " 2.7704615592956543,\n",
       " 2.7818496227264404,\n",
       " 2.7755491733551025,\n",
       " 2.754314422607422,\n",
       " 2.7753801345825195,\n",
       " 2.762403726577759,\n",
       " 2.7736663818359375,\n",
       " 2.774857997894287,\n",
       " 2.788496255874634,\n",
       " 2.7888216972351074,\n",
       " 2.7797794342041016,\n",
       " 2.795207977294922,\n",
       " 2.804119825363159,\n",
       " 2.805851936340332,\n",
       " 2.8118386268615723,\n",
       " 2.8057308197021484,\n",
       " 2.80216383934021,\n",
       " 2.8004348278045654,\n",
       " 2.799049139022827,\n",
       " 2.7915937900543213,\n",
       " 2.7969911098480225,\n",
       " 2.7949960231781006,\n",
       " 2.783374071121216,\n",
       " 2.7769477367401123,\n",
       " 2.7636542320251465,\n",
       " 2.7669944763183594,\n",
       " 2.7594923973083496,\n",
       " 2.7454118728637695,\n",
       " 2.7578518390655518,\n",
       " 2.739683151245117,\n",
       " 2.7534732818603516,\n",
       " 2.757009506225586,\n",
       " 2.741058111190796,\n",
       " 2.7449684143066406,\n",
       " 2.7487642765045166,\n",
       " 2.749983072280884,\n",
       " 2.7406692504882812,\n",
       " 2.7501041889190674,\n",
       " 2.749091863632202,\n",
       " 2.7482991218566895,\n",
       " 2.7396485805511475,\n",
       " 2.7360012531280518,\n",
       " 2.737382411956787,\n",
       " 2.744051694869995,\n",
       " 2.7352967262268066,\n",
       " 2.728219985961914,\n",
       " 2.7288312911987305,\n",
       " 2.7249398231506348,\n",
       " 2.7382678985595703,\n",
       " 2.7369134426116943,\n",
       " 2.728126049041748,\n",
       " 2.736025094985962,\n",
       " 2.7331411838531494,\n",
       " 2.7250139713287354,\n",
       " 2.7268049716949463,\n",
       " 2.7146873474121094,\n",
       " 2.720762252807617,\n",
       " 2.727294921875,\n",
       " 2.7265093326568604,\n",
       " 2.7362799644470215,\n",
       " 2.734830617904663,\n",
       " 2.738642692565918,\n",
       " 2.7385494709014893,\n",
       " 2.749020576477051,\n",
       " 2.749661445617676,\n",
       " 2.345712184906006,\n",
       " 2.177124261856079,\n",
       " 2.3518283367156982,\n",
       " 2.377458333969116,\n",
       " 2.5376057624816895,\n",
       " 2.5853049755096436,\n",
       " 2.6150338649749756,\n",
       " 2.589421272277832,\n",
       " 2.5594053268432617,\n",
       " 2.633934497833252,\n",
       " 2.6472866535186768,\n",
       " 2.6456217765808105,\n",
       " 2.72290301322937,\n",
       " 2.684110641479492,\n",
       " 2.678759813308716,\n",
       " 2.7008652687072754,\n",
       " 2.7162437438964844,\n",
       " 2.7345376014709473,\n",
       " 2.7105956077575684,\n",
       " 2.7116897106170654,\n",
       " 2.7064614295959473,\n",
       " 2.6784117221832275,\n",
       " 2.6937036514282227,\n",
       " 2.6580467224121094,\n",
       " 2.6719985008239746,\n",
       " 2.6657907962799072,\n",
       " 2.6658596992492676,\n",
       " 2.6772572994232178,\n",
       " 2.6893489360809326,\n",
       " 2.7096657752990723,\n",
       " 2.7003862857818604,\n",
       " 2.70226788520813,\n",
       " 2.6999027729034424,\n",
       " 2.6996676921844482,\n",
       " 2.687894105911255,\n",
       " 2.7086551189422607,\n",
       " 2.705061197280884,\n",
       " 2.7061073780059814,\n",
       " 2.700272560119629,\n",
       " 2.7106127738952637,\n",
       " 2.6973841190338135,\n",
       " 2.6919491291046143,\n",
       " 2.6754696369171143,\n",
       " 2.681387424468994,\n",
       " 2.671752452850342,\n",
       " 2.6543080806732178,\n",
       " 2.6624343395233154,\n",
       " 2.6502177715301514,\n",
       " 2.652397394180298,\n",
       " 2.6539218425750732,\n",
       " 2.650592088699341,\n",
       " 2.649726629257202,\n",
       " 2.6680691242218018,\n",
       " 2.6559979915618896,\n",
       " 2.647350311279297,\n",
       " 2.6480350494384766,\n",
       " 2.6616241931915283,\n",
       " 2.6748549938201904,\n",
       " 2.683955192565918,\n",
       " 2.6705899238586426,\n",
       " 2.669198751449585,\n",
       " 2.677823543548584,\n",
       " 2.675283432006836,\n",
       " 2.6690306663513184,\n",
       " 2.6746418476104736,\n",
       " 2.6737735271453857,\n",
       " 2.6864871978759766,\n",
       " 2.6783368587493896,\n",
       " 2.6756725311279297,\n",
       " 2.681532621383667,\n",
       " 2.6937386989593506,\n",
       " 2.6976678371429443,\n",
       " 2.699694871902466,\n",
       " 2.69770884513855,\n",
       " 2.6950974464416504,\n",
       " 2.7019236087799072,\n",
       " 2.705726146697998,\n",
       " 2.710179328918457,\n",
       " 2.7089991569519043,\n",
       " 2.7046689987182617,\n",
       " 2.7011258602142334,\n",
       " 2.699432611465454,\n",
       " 2.696796178817749,\n",
       " 2.7005300521850586,\n",
       " 2.704589366912842,\n",
       " 2.696803569793701,\n",
       " 2.6947357654571533,\n",
       " 2.6956655979156494,\n",
       " 2.689741611480713,\n",
       " 2.6927590370178223,\n",
       " 2.6887285709381104,\n",
       " 2.6807284355163574,\n",
       " 2.684631109237671,\n",
       " 2.6841206550598145,\n",
       " 2.687661647796631,\n",
       " 2.6885569095611572,\n",
       " 2.6873068809509277,\n",
       " 2.6841015815734863,\n",
       " 2.6791815757751465,\n",
       " 2.6838788986206055,\n",
       " 2.104987621307373,\n",
       " 1.9013357162475586,\n",
       " 2.1231634616851807,\n",
       " 2.315359592437744,\n",
       " 2.2447798252105713,\n",
       " 2.3525636196136475,\n",
       " 2.2774338722229004,\n",
       " 2.3317959308624268,\n",
       " 2.453639268875122,\n",
       " 2.553088426589966,\n",
       " 2.6221985816955566,\n",
       " 2.5731539726257324,\n",
       " 2.5819172859191895,\n",
       " 2.5725762844085693,\n",
       " 2.5532448291778564,\n",
       " 2.5654988288879395,\n",
       " 2.598245143890381,\n",
       " 2.570448398590088,\n",
       " 2.605410575866699,\n",
       " 2.5950927734375,\n",
       " 2.6038286685943604,\n",
       " 2.576439142227173,\n",
       " 2.5733742713928223,\n",
       " 2.5925824642181396,\n",
       " 2.5619614124298096,\n",
       " 2.557736396789551,\n",
       " 2.5577547550201416,\n",
       " 2.596243143081665,\n",
       " 2.6068456172943115,\n",
       " 2.609050750732422,\n",
       " 2.585289478302002,\n",
       " 2.5791192054748535,\n",
       " 2.5597641468048096,\n",
       " 2.541508913040161,\n",
       " 2.568406343460083,\n",
       " 2.5602004528045654,\n",
       " 2.542501926422119,\n",
       " 2.5091445446014404,\n",
       " 2.4982120990753174,\n",
       " 2.5154082775115967,\n",
       " 2.5152547359466553,\n",
       " 2.521832227706909,\n",
       " 2.5070791244506836,\n",
       " 2.524493455886841,\n",
       " 2.5352163314819336,\n",
       " 2.532620668411255,\n",
       " 2.5384538173675537,\n",
       " 2.5307741165161133,\n",
       " 2.556525707244873,\n",
       " 2.5572774410247803,\n",
       " 2.567354202270508,\n",
       " 2.562872886657715,\n",
       " 2.55448579788208,\n",
       " 2.5592103004455566,\n",
       " 2.579364776611328,\n",
       " 2.5937488079071045,\n",
       " 2.593665599822998,\n",
       " 2.5906076431274414,\n",
       " 2.5858280658721924,\n",
       " 2.5902225971221924,\n",
       " 2.6010591983795166,\n",
       " 2.5968375205993652,\n",
       " 2.6003265380859375,\n",
       " 2.6035847663879395,\n",
       " 2.6026129722595215,\n",
       " 2.5994324684143066,\n",
       " 2.5958762168884277,\n",
       " 2.5993218421936035,\n",
       " 2.6002020835876465,\n",
       " 2.5980355739593506,\n",
       " 2.598372220993042,\n",
       " 2.617640972137451,\n",
       " 2.6181278228759766,\n",
       " 2.6152732372283936,\n",
       " 2.612811803817749,\n",
       " 2.608980417251587,\n",
       " 2.6104118824005127,\n",
       " 2.608290433883667,\n",
       " 2.6034955978393555,\n",
       " 2.601065158843994,\n",
       " 2.5920872688293457,\n",
       " 2.6016952991485596,\n",
       " 2.6001133918762207,\n",
       " 2.597177505493164,\n",
       " 2.590456008911133,\n",
       " 2.599813461303711,\n",
       " 2.5916123390197754,\n",
       " 2.599754571914673,\n",
       " 2.6058754920959473,\n",
       " 2.610379219055176,\n",
       " 2.608856439590454,\n",
       " 2.612762212753296,\n",
       " 2.6057491302490234,\n",
       " 2.6072821617126465,\n",
       " 2.6077139377593994,\n",
       " 2.6167757511138916,\n",
       " 2.6176202297210693,\n",
       " 2.6183128356933594,\n",
       " 2.6187937259674072,\n",
       " 2.6148154735565186,\n",
       " 1.8826003074645996,\n",
       " 1.470383882522583,\n",
       " 1.8479557037353516,\n",
       " 1.9969520568847656,\n",
       " 2.1157233715057373,\n",
       " 2.1371002197265625,\n",
       " 2.2848398685455322,\n",
       " 2.320556640625,\n",
       " 2.359034538269043,\n",
       " 2.3230252265930176,\n",
       " 2.3264107704162598,\n",
       " 2.357086420059204,\n",
       " 2.316782236099243,\n",
       " 2.3341431617736816,\n",
       " 2.308101177215576,\n",
       " 2.3036179542541504,\n",
       " 2.3175861835479736,\n",
       " 2.3274829387664795,\n",
       " 2.351073980331421,\n",
       " 2.3948521614074707,\n",
       " 2.396759510040283,\n",
       " 2.406371831893921,\n",
       " 2.4161629676818848,\n",
       " 2.4310855865478516,\n",
       " 2.432943344116211,\n",
       " 2.467451572418213,\n",
       " 2.4548237323760986,\n",
       " 2.4369618892669678,\n",
       " 2.434356927871704,\n",
       " 2.447600841522217,\n",
       " 2.4320428371429443,\n",
       " 2.461236000061035,\n",
       " 2.455444097518921,\n",
       " 2.435835599899292,\n",
       " 2.448037624359131,\n",
       " 2.4543097019195557,\n",
       " 2.4597251415252686,\n",
       " 2.4739391803741455,\n",
       " 2.4704346656799316,\n",
       " 2.4575867652893066,\n",
       " 2.469986915588379,\n",
       " 2.4762051105499268,\n",
       " 2.499044179916382,\n",
       " 2.48451828956604,\n",
       " 2.4781253337860107,\n",
       " 2.4762489795684814,\n",
       " 2.4955432415008545,\n",
       " 2.491408109664917,\n",
       " 2.490530252456665,\n",
       " 2.5087668895721436,\n",
       " 2.5219831466674805,\n",
       " 2.5244827270507812,\n",
       " 2.5188586711883545,\n",
       " 2.533008098602295,\n",
       " 2.5366995334625244,\n",
       " 2.5275228023529053,\n",
       " 2.526337146759033,\n",
       " 2.522489547729492,\n",
       " 2.5314271450042725,\n",
       " 2.5383660793304443,\n",
       " 2.547107458114624,\n",
       " 2.554020404815674,\n",
       " 2.552489757537842,\n",
       " 2.5522446632385254,\n",
       " 2.5651071071624756,\n",
       " 2.564368963241577,\n",
       " 2.5620498657226562,\n",
       " 2.561581611633301,\n",
       " 2.566087484359741,\n",
       " 2.566619634628296,\n",
       " 2.574272632598877,\n",
       " 2.575056314468384,\n",
       " 2.5676827430725098,\n",
       " 2.5763702392578125,\n",
       " 2.5652520656585693,\n",
       " 2.5691158771514893,\n",
       " 2.5681724548339844,\n",
       " 2.5637264251708984,\n",
       " 2.5517194271087646,\n",
       " 2.5612521171569824,\n",
       " 2.552682638168335,\n",
       " 2.5490477085113525,\n",
       " 2.5392515659332275,\n",
       " 2.5363283157348633,\n",
       " 2.5366649627685547,\n",
       " 2.534130096435547,\n",
       " 2.542022705078125,\n",
       " 2.546292543411255,\n",
       " 2.552432060241699,\n",
       " 2.545595645904541,\n",
       " 2.542337656021118,\n",
       " 2.5539040565490723,\n",
       " 2.5508174896240234,\n",
       " 2.535909652709961,\n",
       " 2.576186418533325,\n",
       " 2.5748770236968994,\n",
       " 2.5687241554260254,\n",
       " 2.566641330718994,\n",
       " 2.5604851245880127,\n",
       " 2.564237356185913,\n",
       " 1.8493678569793701,\n",
       " 2.5396387577056885,\n",
       " 2.449773073196411,\n",
       " 2.218468189239502,\n",
       " 2.368292808532715,\n",
       " 2.636504888534546,\n",
       " 2.748058557510376,\n",
       " 2.62864089012146,\n",
       " 2.5948753356933594,\n",
       " 2.613612651824951,\n",
       " 2.577253580093384,\n",
       " 2.514694929122925,\n",
       " 2.5932726860046387,\n",
       " 2.630126953125,\n",
       " 2.5964198112487793,\n",
       " 2.6028618812561035,\n",
       " 2.5822229385375977,\n",
       " 2.5977087020874023,\n",
       " 2.5826847553253174,\n",
       " 2.5897810459136963,\n",
       " 2.581435441970825,\n",
       " 2.567368745803833,\n",
       " 2.5571486949920654,\n",
       " 2.5749881267547607,\n",
       " 2.580650568008423,\n",
       " 2.5789201259613037,\n",
       " 2.567589044570923,\n",
       " 2.5643012523651123,\n",
       " 2.538806676864624,\n",
       " 2.525851011276245,\n",
       " 2.5511691570281982,\n",
       " 2.5773003101348877,\n",
       " 2.5795629024505615,\n",
       " 2.618908643722534,\n",
       " 2.6041553020477295,\n",
       " 2.607398271560669,\n",
       " 2.6153509616851807,\n",
       " 2.6168956756591797,\n",
       " 2.609804153442383,\n",
       " 2.609668254852295,\n",
       " 2.635327100753784,\n",
       " 2.629190444946289,\n",
       " 2.6190502643585205,\n",
       " 2.599241018295288,\n",
       " 2.613163471221924,\n",
       " 2.6139183044433594,\n",
       " 2.6077842712402344,\n",
       " 2.619048595428467,\n",
       " 2.622856616973877,\n",
       " 2.640162467956543,\n",
       " 2.633181095123291,\n",
       " 2.640599250793457,\n",
       " 2.645246982574463,\n",
       " 2.6365575790405273,\n",
       " 2.6309406757354736,\n",
       " 2.616248607635498,\n",
       " 2.619044780731201,\n",
       " 2.621209144592285,\n",
       " 2.6153321266174316,\n",
       " 2.602318286895752,\n",
       " 2.5981340408325195,\n",
       " 2.5828001499176025,\n",
       " 2.5771501064300537,\n",
       " 2.55830454826355,\n",
       " 2.578021287918091,\n",
       " 2.5838985443115234,\n",
       " 2.5810093879699707,\n",
       " 2.597625255584717,\n",
       " 2.5988411903381348,\n",
       " 2.6006340980529785,\n",
       " 2.596153736114502,\n",
       " 2.5984084606170654,\n",
       " 2.6043858528137207,\n",
       " 2.6015124320983887,\n",
       " 2.598975419998169,\n",
       " 2.592723846435547,\n",
       " 2.5937962532043457,\n",
       " 2.5858781337738037,\n",
       " 2.593672752380371,\n",
       " 2.5914649963378906,\n",
       " 2.5938026905059814,\n",
       " 2.593892812728882,\n",
       " 2.603215217590332,\n",
       " 2.598689317703247,\n",
       " 2.597449541091919,\n",
       " 2.6032357215881348,\n",
       " 2.6113152503967285,\n",
       " 2.6101131439208984,\n",
       " 2.6089701652526855,\n",
       " 2.6119725704193115,\n",
       " 2.6148016452789307,\n",
       " 2.6088802814483643,\n",
       " 2.60842227935791,\n",
       " 2.6165759563446045,\n",
       " 2.6161625385284424,\n",
       " 2.6106550693511963,\n",
       " 2.61350417137146,\n",
       " 2.6134965419769287,\n",
       " 2.6148767471313477,\n",
       " 2.6116397380828857,\n",
       " 1.7183003425598145,\n",
       " 2.2160072326660156,\n",
       " 2.113783597946167,\n",
       " 2.184652090072632,\n",
       " 2.4013283252716064,\n",
       " 2.3658554553985596,\n",
       " 2.3890271186828613,\n",
       " 2.336453676223755,\n",
       " 2.358614444732666,\n",
       " 2.4161486625671387,\n",
       " 2.4738316535949707,\n",
       " 2.4310622215270996,\n",
       " 2.453611135482788,\n",
       " 2.4199531078338623,\n",
       " 2.3599770069122314,\n",
       " 2.4090428352355957,\n",
       " 2.4229736328125,\n",
       " 2.4496331214904785,\n",
       " 2.438802719116211,\n",
       " 2.4366724491119385,\n",
       " 2.5009677410125732,\n",
       " 2.492893934249878,\n",
       " 2.4733850955963135,\n",
       " 2.4498045444488525,\n",
       " 2.4543638229370117,\n",
       " 2.4414422512054443,\n",
       " 2.420055866241455,\n",
       " 2.4026057720184326,\n",
       " 2.4318747520446777,\n",
       " 2.429816484451294,\n",
       " 2.4311368465423584,\n",
       " 2.418814182281494,\n",
       " 2.432940721511841,\n",
       " 2.453178644180298,\n",
       " 2.452765941619873,\n",
       " 2.4284422397613525,\n",
       " 2.4064815044403076,\n",
       " 2.4546115398406982,\n",
       " 2.4512104988098145,\n",
       " 2.466489315032959,\n",
       " 2.4682199954986572,\n",
       " 2.458422899246216,\n",
       " 2.469592809677124,\n",
       " 2.472796678543091,\n",
       " 2.474400043487549,\n",
       " 2.458904981613159,\n",
       " 2.4557881355285645,\n",
       " 2.4479782581329346,\n",
       " 2.45969557762146,\n",
       " 2.4715261459350586,\n",
       " 2.4737164974212646,\n",
       " 2.4529378414154053,\n",
       " 2.4381520748138428,\n",
       " 2.4354593753814697,\n",
       " 2.4234511852264404,\n",
       " 2.4280295372009277,\n",
       " 2.418982982635498,\n",
       " 2.424257278442383,\n",
       " 2.422363042831421,\n",
       " 2.4276843070983887,\n",
       " 2.4507524967193604,\n",
       " 2.4483373165130615,\n",
       " 2.4400408267974854,\n",
       " 2.4406683444976807,\n",
       " 2.4579710960388184,\n",
       " 2.452113151550293,\n",
       " 2.4577724933624268,\n",
       " 2.4597725868225098,\n",
       " 2.452937364578247,\n",
       " 2.4649946689605713,\n",
       " 2.456923723220825,\n",
       " 2.4657764434814453,\n",
       " 2.4771437644958496,\n",
       " 2.471571207046509,\n",
       " 2.459664821624756,\n",
       " 2.446648359298706,\n",
       " 2.4430179595947266,\n",
       " 2.444711923599243,\n",
       " 2.4507369995117188,\n",
       " 2.4500250816345215,\n",
       " 2.4524083137512207,\n",
       " 2.463141441345215,\n",
       " 2.46836256980896,\n",
       " 2.477370500564575,\n",
       " 2.482605457305908,\n",
       " 2.486981153488159,\n",
       " 2.483947992324829,\n",
       " 2.4875028133392334,\n",
       " 2.4915847778320312,\n",
       " 2.4906811714172363,\n",
       " 2.49932599067688,\n",
       " 2.501746892929077,\n",
       " 2.5073752403259277,\n",
       " 2.5103001594543457,\n",
       " 2.5200536251068115,\n",
       " 2.5160775184631348,\n",
       " 2.516836166381836,\n",
       " 2.5181853771209717,\n",
       " 2.5175180435180664,\n",
       " 2.5204336643218994,\n",
       " 2.0279228687286377,\n",
       " 2.3159728050231934,\n",
       " 2.400195360183716,\n",
       " 2.36198353767395,\n",
       " 2.4630863666534424,\n",
       " 2.5080649852752686,\n",
       " 2.5320241451263428,\n",
       " 2.5131635665893555,\n",
       " 2.503309726715088,\n",
       " 2.5089526176452637,\n",
       " 2.5071282386779785,\n",
       " 2.520921468734741,\n",
       " 2.4858500957489014,\n",
       " 2.48555588722229,\n",
       " 2.4687552452087402,\n",
       " 2.4507524967193604,\n",
       " 2.462460994720459,\n",
       " 2.4500763416290283,\n",
       " 2.4260833263397217,\n",
       " 2.40505313873291,\n",
       " 2.3713181018829346,\n",
       " 2.3500478267669678,\n",
       " 2.4095330238342285,\n",
       " 2.401693105697632,\n",
       " 2.3768796920776367,\n",
       " 2.3688111305236816,\n",
       " 2.3636913299560547,\n",
       " 2.3689942359924316,\n",
       " 2.390002489089966,\n",
       " 2.419862985610962,\n",
       " 2.423121929168701,\n",
       " 2.398324728012085,\n",
       " 2.391904592514038,\n",
       " 2.3834927082061768,\n",
       " 2.3954923152923584,\n",
       " 2.384634256362915,\n",
       " 2.376411199569702,\n",
       " 2.3708815574645996,\n",
       " 2.346926689147949,\n",
       " 2.3882808685302734,\n",
       " 2.397737503051758,\n",
       " 2.39172625541687,\n",
       " 2.406637191772461,\n",
       " 2.4248135089874268,\n",
       " 2.4269590377807617,\n",
       " 2.439866065979004,\n",
       " 2.437107801437378,\n",
       " 2.4199159145355225,\n",
       " 2.4419007301330566,\n",
       " 2.4429688453674316,\n",
       " 2.4529690742492676,\n",
       " 2.4543914794921875,\n",
       " 2.450826406478882,\n",
       " 2.4606246948242188,\n",
       " 2.4651403427124023,\n",
       " 2.4719483852386475,\n",
       " 2.4647324085235596,\n",
       " 2.4674346446990967,\n",
       " 2.4825809001922607,\n",
       " 2.480705738067627,\n",
       " 2.4823861122131348,\n",
       " 2.4710965156555176,\n",
       " 2.455958366394043,\n",
       " 2.47139048576355,\n",
       " 2.468270778656006,\n",
       " 2.4800233840942383,\n",
       " 2.4812424182891846,\n",
       " 2.4870529174804688,\n",
       " 2.474872589111328,\n",
       " 2.4665098190307617,\n",
       " 2.465440273284912,\n",
       " 2.471156597137451,\n",
       " 2.4785690307617188,\n",
       " 2.487088918685913,\n",
       " 2.4951155185699463,\n",
       " 2.4966888427734375,\n",
       " 2.5092737674713135,\n",
       " 2.4999818801879883,\n",
       " 2.4873428344726562,\n",
       " 2.489013433456421,\n",
       " 2.482787609100342,\n",
       " 2.485020637512207,\n",
       " 2.484093427658081,\n",
       " 2.4799623489379883,\n",
       " 2.4859204292297363,\n",
       " 2.4742815494537354,\n",
       " 2.4791481494903564,\n",
       " 2.4824838638305664,\n",
       " 2.482945442199707,\n",
       " 2.481358766555786,\n",
       " 2.4825515747070312,\n",
       " 2.47141695022583,\n",
       " 2.471691370010376,\n",
       " 2.4711556434631348,\n",
       " 2.468749523162842,\n",
       " 2.4667980670928955,\n",
       " 2.472667694091797,\n",
       " 2.4792630672454834,\n",
       " 2.4873101711273193,\n",
       " 2.489293336868286,\n",
       " 1.8672071695327759,\n",
       " 1.8328831195831299,\n",
       " 1.7863613367080688,\n",
       " 1.802234411239624,\n",
       " 1.9167120456695557,\n",
       " 2.0388004779815674,\n",
       " 2.12131667137146,\n",
       " 2.1809029579162598,\n",
       " 2.1347804069519043,\n",
       " 2.239161252975464,\n",
       " 2.256452798843384,\n",
       " 2.307013988494873,\n",
       " 2.296699285507202,\n",
       " 2.3191475868225098,\n",
       " 2.3260388374328613,\n",
       " 2.318040132522583,\n",
       " 2.337219476699829,\n",
       " 2.3489277362823486,\n",
       " 2.311178684234619,\n",
       " 2.366055727005005,\n",
       " 2.379932403564453,\n",
       " 2.3511083126068115,\n",
       " 2.3829855918884277,\n",
       " 2.3396174907684326,\n",
       " 2.3495404720306396,\n",
       " 2.3288040161132812,\n",
       " 2.3141918182373047,\n",
       " 2.3475229740142822,\n",
       " 2.3164424896240234,\n",
       " 2.3238205909729004,\n",
       " 2.2969272136688232,\n",
       " 2.315918445587158,\n",
       " 2.3238420486450195,\n",
       " 2.342446804046631,\n",
       " 2.3513898849487305,\n",
       " 2.3614728450775146,\n",
       " 2.378469228744507,\n",
       " 2.389638662338257,\n",
       " 2.3905537128448486,\n",
       " 2.3789963722229004,\n",
       " 2.3723132610321045,\n",
       " 2.3796846866607666,\n",
       " 2.382673501968384,\n",
       " 2.3855810165405273,\n",
       " 2.3828587532043457,\n",
       " 2.3513996601104736,\n",
       " 2.3492555618286133,\n",
       " 2.3516294956207275,\n",
       " 2.3642709255218506,\n",
       " 2.383931875228882,\n",
       " 2.3739092350006104,\n",
       " 2.3854269981384277,\n",
       " 2.384953260421753,\n",
       " 2.402639389038086,\n",
       " 2.3928916454315186,\n",
       " 2.3765861988067627,\n",
       " 2.3708794116973877,\n",
       " 2.36076283454895,\n",
       " 2.372789144515991,\n",
       " 2.373875856399536,\n",
       " 2.386838674545288,\n",
       " 2.37107515335083,\n",
       " 2.371652126312256,\n",
       " 2.3516886234283447,\n",
       " 2.353508949279785,\n",
       " 2.3538687229156494,\n",
       " 2.351844310760498,\n",
       " 2.350027322769165,\n",
       " 2.3587100505828857,\n",
       " 2.3436384201049805,\n",
       " 2.338848114013672,\n",
       " 2.3395891189575195,\n",
       " 2.3373262882232666,\n",
       " 2.328911781311035,\n",
       " 2.3449454307556152,\n",
       " 2.3469178676605225,\n",
       " 2.3429388999938965,\n",
       " 2.3446755409240723,\n",
       " 2.3408141136169434,\n",
       " 2.351280927658081,\n",
       " 2.352954387664795,\n",
       " 2.3625853061676025,\n",
       " 2.376711368560791,\n",
       " 2.3652071952819824,\n",
       " 2.3667309284210205,\n",
       " 2.377211809158325,\n",
       " 2.385943651199341,\n",
       " 2.381655693054199,\n",
       " 2.3795480728149414,\n",
       " 2.383833408355713,\n",
       " 2.3799991607666016,\n",
       " 2.37917160987854,\n",
       " 2.379725694656372,\n",
       " 2.3767027854919434,\n",
       " 2.375653028488159,\n",
       " 2.385427474975586,\n",
       " 2.389341354370117,\n",
       " 2.3948843479156494,\n",
       " 2.3868417739868164,\n",
       " 2.379525899887085,\n",
       " 2.467578649520874,\n",
       " 1.7988643646240234,\n",
       " 1.6929092407226562,\n",
       " 1.9569143056869507,\n",
       " 1.886151909828186,\n",
       " 2.128739356994629,\n",
       " 2.1802356243133545,\n",
       " 2.233783006668091,\n",
       " 2.1078250408172607,\n",
       " 2.0520405769348145,\n",
       " 2.164424180984497,\n",
       " 2.1407101154327393,\n",
       " 2.214263439178467,\n",
       " 2.323887586593628,\n",
       " 2.359971046447754,\n",
       " 2.329427719116211,\n",
       " 2.346132755279541,\n",
       " 2.3679630756378174,\n",
       " 2.3103768825531006,\n",
       " 2.329119920730591,\n",
       " 2.325908899307251,\n",
       " 2.361964225769043,\n",
       " 2.372457504272461,\n",
       " 2.403597831726074,\n",
       " 2.4167284965515137,\n",
       " 2.411912202835083,\n",
       " 2.4306836128234863,\n",
       " 2.4351117610931396,\n",
       " 2.4345853328704834,\n",
       " 2.423452854156494,\n",
       " 2.436957359313965,\n",
       " 2.4509663581848145,\n",
       " 2.4450385570526123,\n",
       " 2.426860809326172,\n",
       " 2.4208078384399414,\n",
       " 2.4428606033325195,\n",
       " 2.4290506839752197,\n",
       " 2.436202049255371,\n",
       " 2.436309337615967,\n",
       " 2.4370031356811523,\n",
       " 2.439359664916992,\n",
       " 2.460660934448242,\n",
       " 2.460315704345703,\n",
       " 2.4539928436279297,\n",
       " 2.447439432144165,\n",
       " 2.445291042327881,\n",
       " 2.4321320056915283,\n",
       " 2.4368457794189453,\n",
       " 2.4314937591552734,\n",
       " 2.423567295074463,\n",
       " 2.4321935176849365,\n",
       " 2.415222644805908,\n",
       " 2.4163546562194824,\n",
       " 2.4098620414733887,\n",
       " 2.386802911758423,\n",
       " 2.3887760639190674,\n",
       " 2.363656997680664,\n",
       " 2.3701934814453125,\n",
       " 2.3689892292022705,\n",
       " 2.384105920791626,\n",
       " 2.382963180541992,\n",
       " 2.3799521923065186,\n",
       " 2.3757405281066895,\n",
       " 2.375051975250244,\n",
       " 2.3852157592773438,\n",
       " 2.3907175064086914,\n",
       " 2.383646011352539,\n",
       " 2.375352382659912,\n",
       " 2.377654552459717,\n",
       " 2.373023509979248,\n",
       " 2.389040946960449,\n",
       " 2.3965139389038086,\n",
       " 2.376899480819702,\n",
       " 2.3575994968414307,\n",
       " 2.3624932765960693,\n",
       " 2.367440938949585,\n",
       " 2.3613224029541016,\n",
       " 2.3572840690612793,\n",
       " 2.361736536026001,\n",
       " 2.3424019813537598,\n",
       " 2.3546600341796875,\n",
       " 2.3346264362335205,\n",
       " 2.342324733734131,\n",
       " 2.3475749492645264,\n",
       " 2.3480398654937744,\n",
       " 2.3487749099731445,\n",
       " 2.3529839515686035,\n",
       " 2.368894577026367,\n",
       " 2.3594698905944824,\n",
       " 2.3623859882354736,\n",
       " 2.355400562286377,\n",
       " 2.3631784915924072,\n",
       " 2.356632947921753,\n",
       " 2.3540632724761963,\n",
       " 2.3511767387390137,\n",
       " 2.3616979122161865,\n",
       " 2.353910446166992,\n",
       " 2.358463764190674,\n",
       " 2.373063325881958,\n",
       " 2.361982583999634]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.losses_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e9b9be-2f1b-47d6-9ca8-0c1e620102f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3efa10-06fb-4b0b-8079-091f41956d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
